"""
Desafio 5 - RecomendaÃ§Ã£o de PrÃ³ximas Habilidades

Objetivo: Dado um perfil atual e horizonte de 5 anos, sugerir as prÃ³ximas 2-3
habilidades que maximizam o valor esperado, considerando transiÃ§Ãµes de mercado.

CenÃ¡rios de Mercado (prÃ³ximos 5 anos):
    1. IA em Alta (35%): Boost em IA Generativa, ML, Big Data
    2. Cloud First (30%): Boost em Cloud, DevOps, APIs
    3. Data Driven (20%): Boost em Dados, SQL, BI, Big Data
    4. SeguranÃ§a CrÃ­tica (15%): Boost em SeguranÃ§a, Cloud, DevOps

Algoritmo:
    DP com Look-Ahead em horizonte finito (5 anos)
    
    Estado: (skills_adquiridas, ano_atual)
    AÃ§Ã£o: Escolher prÃ³xima habilidade
    TransiÃ§Ã£o: Probabilidades de cenÃ¡rio de mercado
    
    V(S, t) = max_{a} [ E[Valor(a) | cenÃ¡rios] + V(S âˆª {a}, t+1) ]
    
    Onde:
    - S = conjunto de skills jÃ¡ adquiridas
    - t = ano atual (0 a 5)
    - a = prÃ³xima skill a adquirir
    - E[Valor(a) | cenÃ¡rios] = valor esperado considerando probabilidades

Complexidade:
    O(n Ã— 2^n Ã— h) onde n = skills disponÃ­veis, h = horizonte
    Para n=12, h=5: O(12 Ã— 4096 Ã— 5) â‰ˆ O(245,760)
    
    Na prÃ¡tica, usamos aproximaÃ§Ãµes:
    - Limitar profundidade de busca
    - Considerar apenas k prÃ³ximas melhores skills
    - Usar heurÃ­stica gulosa com look-ahead
"""

from typing import List, Dict, Tuple, Set, Any, Optional
from dataclasses import dataclass
import numpy as np
from itertools import combinations

from src.graph_structures import SkillGraph, build_graph_from_file
from src.decorators import measure_performance
from src.config import MARKET_SCENARIOS


# Horizonte de planejamento
RECOMMENDATION_HORIZON_YEARS = 5

# NÃºmero de habilidades a recomendar
N_RECOMMENDATIONS = 3


@dataclass
class MarketScenario:
    """
    CenÃ¡rio de mercado futuro.
    
    Attributes:
        name: Nome do cenÃ¡rio
        probability: Probabilidade de ocorrÃªncia (0-1)
        boosts: Multiplicadores de valor por skill {skill_id: multiplier}
        description: DescriÃ§Ã£o do cenÃ¡rio
    """
    name: str
    probability: float
    boosts: Dict[str, float]
    description: str
    
    def apply_boost(self, skill_id: str, base_value: float) -> float:
        """
        Aplica boost do cenÃ¡rio ao valor base.
        
        Args:
            skill_id: ID da habilidade
            base_value: Valor base
        
        Returns:
            float: Valor com boost aplicado
        """
        multiplier = self.boosts.get(skill_id, 1.0)
        return base_value * multiplier


@dataclass
class Recommendation:
    """
    RecomendaÃ§Ã£o de habilidades.
    
    Attributes:
        skills_recommended: Lista de IDs das habilidades recomendadas
        expected_value: Valor esperado total
        expected_value_per_scenario: Valor esperado por cenÃ¡rio
        reasoning: Justificativa da recomendaÃ§Ã£o
        details: Detalhes de cada habilidade recomendada
    """
    skills_recommended: List[str]
    expected_value: float
    expected_value_per_scenario: Dict[str, float]
    reasoning: str
    details: List[Dict[str, Any]]
    
    def to_dict(self) -> Dict[str, Any]:
        """Converte para dicionÃ¡rio."""
        return {
            'skills_recommended': self.skills_recommended,
            'expected_value': self.expected_value,
            'expected_value_per_scenario': self.expected_value_per_scenario,
            'reasoning': self.reasoning,
            'details': self.details
        }
    
    def __str__(self) -> str:
        """RepresentaÃ§Ã£o string formatada."""
        skills_str = ' â†’ '.join(self.skills_recommended)
        return (
            f"Recommendation:\n"
            f"  Skills: {skills_str}\n"
            f"  E[Valor] = {self.expected_value:.2f}\n"
            f"  {self.reasoning}"
        )


def load_market_scenarios() -> List[MarketScenario]:
    """
    Carrega cenÃ¡rios de mercado da configuraÃ§Ã£o.
    
    Returns:
        List[MarketScenario]: Lista de cenÃ¡rios
    """
    scenarios = []
    
    for scenario_name, scenario_data in MARKET_SCENARIOS.items():
        scenario = MarketScenario(
            name=scenario_name,
            probability=scenario_data['prob'],
            boosts=scenario_data['boost'],
            description=f"CenÃ¡rio: {scenario_name.replace('_', ' ').title()}"
        )
        scenarios.append(scenario)
    
    return scenarios


def calculate_expected_value(
    skill_id: str,
    base_value: float,
    scenarios: List[MarketScenario]
) -> Tuple[float, Dict[str, float]]:
    """
    Calcula valor esperado de uma habilidade considerando cenÃ¡rios de mercado.
    
    E[Valor] = Î£ P(cenÃ¡rio) Ã— Valor(skill | cenÃ¡rio)
    
    Args:
        skill_id: ID da habilidade
        base_value: Valor base da habilidade
        scenarios: Lista de cenÃ¡rios de mercado
    
    Returns:
        Tuple[float, Dict]: (valor_esperado, valores_por_cenario)
    
    Examples:
        >>> expected, per_scenario = calculate_expected_value('S6', 10, scenarios)
        >>> # Se S6 tem boost em "ia_em_alta" (35%, +25%):
        >>> # E[V] = 0.35 Ã— 12.5 + 0.30 Ã— 10 + 0.20 Ã— 10 + 0.15 Ã— 10 = 10.375
    """
    values_per_scenario = {}
    expected_value = 0.0
    
    for scenario in scenarios:
        # Aplica boost do cenÃ¡rio
        boosted_value = scenario.apply_boost(skill_id, base_value)
        values_per_scenario[scenario.name] = boosted_value
        
        # Pondera pela probabilidade
        expected_value += scenario.probability * boosted_value
    
    return expected_value, values_per_scenario


def greedy_recommendation_with_lookahead(
    graph: SkillGraph,
    current_skills: Set[str],
    n_recommendations: int = N_RECOMMENDATIONS,
    scenarios: Optional[List[MarketScenario]] = None,
    lookahead_depth: int = 2
) -> Recommendation:
    """
    Recomenda habilidades usando algoritmo guloso com look-ahead.
    
    Algoritmo:
        1. Para cada skill disponÃ­vel (nÃ£o adquirida):
           a. Calcula valor esperado imediato (considerando cenÃ¡rios)
           b. Simula aquisiÃ§Ã£o e calcula valor das prÃ³ximas k skills
           c. Score = valor_imediato + Î± Ã— valor_futuro
        2. Seleciona as top n_recommendations skills com maior score
    
    Args:
        graph: Grafo de habilidades
        current_skills: Conjunto de skills jÃ¡ adquiridas
        n_recommendations: NÃºmero de skills a recomendar
        scenarios: Lista de cenÃ¡rios de mercado
        lookahead_depth: Profundidade do look-ahead (1-3)
    
    Returns:
        Recommendation: RecomendaÃ§Ã£o completa
    
    Complexity:
        O(n Ã— k Ã— m) onde:
        - n = skills disponÃ­veis
        - k = lookahead_depth
        - m = nÃºmero de cenÃ¡rios
    """
    if scenarios is None:
        scenarios = load_market_scenarios()
    
    # Skills disponÃ­veis (nÃ£o adquiridas e com prÃ©-reqs satisfeitos)
    available_skills = []
    for skill_id in graph.nodes:
        if skill_id in current_skills:
            continue
        
        # Verifica prÃ©-requisitos
        prereqs = graph.get_prerequisites(skill_id)
        if all(prereq in current_skills for prereq in prereqs):
            available_skills.append(skill_id)
    
    if not available_skills:
        return Recommendation(
            skills_recommended=[],
            expected_value=0.0,
            expected_value_per_scenario={},
            reasoning="Nenhuma habilidade disponÃ­vel (prÃ©-requisitos nÃ£o satisfeitos)",
            details=[]
        )
    
    # Calcula score para cada skill disponÃ­vel
    skill_scores = []
    
    for skill_id in available_skills:
        metadata = graph.get_metadata(skill_id)
        base_value = metadata['valor']
        
        # Valor esperado imediato
        immediate_value, values_per_scenario = calculate_expected_value(
            skill_id, base_value, scenarios
        )
        
        # Look-ahead: simula aquisiÃ§Ã£o e calcula valor futuro
        future_value = 0.0
        if lookahead_depth > 0:
            # Simula conjunto com skill adquirida
            temp_acquired = current_skills | {skill_id}
            
            # Calcula valor esperado das prÃ³ximas skills acessÃ­veis
            future_available = []
            for next_skill_id in graph.nodes:
                if next_skill_id in temp_acquired:
                    continue
                
                next_prereqs = graph.get_prerequisites(next_skill_id)
                if all(prereq in temp_acquired for prereq in next_prereqs):
                    next_metadata = graph.get_metadata(next_skill_id)
                    next_base_value = next_metadata['valor']
                    next_expected, _ = calculate_expected_value(
                        next_skill_id, next_base_value, scenarios
                    )
                    future_available.append(next_expected)
            
            # Valor futuro = mÃ©dia das top k skills futuras
            if future_available:
                future_available.sort(reverse=True)
                top_k = min(lookahead_depth, len(future_available))
                future_value = sum(future_available[:top_k]) / top_k
        
        # Score combinado: valor imediato + fator de desconto Ã— valor futuro
        discount_factor = 0.3  # Peso do futuro vs presente
        total_score = immediate_value + discount_factor * future_value
        
        skill_scores.append({
            'skill_id': skill_id,
            'nome': metadata['nome'],
            'immediate_value': immediate_value,
            'future_value': future_value,
            'total_score': total_score,
            'values_per_scenario': values_per_scenario,
            'base_value': base_value,
            'tempo_horas': metadata['tempo_horas']
        })
    
    # Ordena por score total (decrescente)
    skill_scores.sort(key=lambda x: x['total_score'], reverse=True)
    
    # Seleciona top n_recommendations
    top_recommendations = skill_scores[:n_recommendations]
    
    # Calcula valor esperado total
    total_expected_value = sum(rec['immediate_value'] for rec in top_recommendations)
    
    # Agrega valores por cenÃ¡rio
    aggregated_per_scenario = {scenario.name: 0.0 for scenario in scenarios}
    for rec in top_recommendations:
        for scenario_name, value in rec['values_per_scenario'].items():
            aggregated_per_scenario[scenario_name] += value
    
    # Gera justificativa
    reasoning = generate_reasoning(top_recommendations, scenarios)
    
    # Detalhes
    details = []
    for rec in top_recommendations:
        details.append({
            'skill_id': rec['skill_id'],
            'nome': rec['nome'],
            'immediate_value': rec['immediate_value'],
            'future_value': rec['future_value'],
            'total_score': rec['total_score'],
            'base_value': rec['base_value'],
            'tempo_horas': rec['tempo_horas'],
            'values_per_scenario': rec['values_per_scenario']
        })
    
    return Recommendation(
        skills_recommended=[rec['skill_id'] for rec in top_recommendations],
        expected_value=total_expected_value,
        expected_value_per_scenario=aggregated_per_scenario,
        reasoning=reasoning,
        details=details
    )


def generate_reasoning(
    recommendations: List[Dict[str, Any]],
    scenarios: List[MarketScenario]
) -> str:
    """
    Gera justificativa textual para as recomendaÃ§Ãµes.
    
    Args:
        recommendations: Lista de recomendaÃ§Ãµes
        scenarios: Lista de cenÃ¡rios
    
    Returns:
        str: Justificativa formatada
    """
    if not recommendations:
        return "Nenhuma recomendaÃ§Ã£o disponÃ­vel."
    
    lines = ["RecomendaÃ§Ã£o baseada em:"]
    
    # Identifica cenÃ¡rio mais provÃ¡vel
    most_likely_scenario = max(scenarios, key=lambda s: s.probability)
    lines.append(f"â€¢ CenÃ¡rio mais provÃ¡vel: {most_likely_scenario.name} ({most_likely_scenario.probability*100:.0f}%)")
    
    # Analisa recomendaÃ§Ãµes
    skill_names = [rec['nome'] for rec in recommendations]
    lines.append(f"â€¢ Skills recomendadas alinham com tendÃªncias de mercado")
    
    # Identifica skills com maior boost
    best_boosts = []
    for rec in recommendations:
        for scenario in scenarios:
            if rec['skill_id'] in scenario.boosts:
                boost = scenario.boosts[rec['skill_id']]
                if boost > 1.1:  # Boost significativo (>10%)
                    best_boosts.append(
                        f"{rec['skill_id']} (+{(boost-1)*100:.0f}% em {scenario.name})"
                    )
    
    if best_boosts:
        lines.append(f"â€¢ Boosts identificados: {', '.join(best_boosts[:3])}")
    
    return '\n'.join(lines)


def dp_recommendation_exhaustive(
    graph: SkillGraph,
    current_skills: Set[str],
    n_recommendations: int = N_RECOMMENDATIONS,
    scenarios: Optional[List[MarketScenario]] = None,
    max_combinations: int = 1000
) -> Recommendation:
    """
    Recomenda habilidades usando DP exaustivo (forÃ§a bruta otimizada).
    
    Enumera todas as combinaÃ§Ãµes possÃ­veis de n habilidades e escolhe
    a que maximiza o valor esperado.
    
    Args:
        graph: Grafo de habilidades
        current_skills: Conjunto de skills jÃ¡ adquiridas
        n_recommendations: NÃºmero de skills a recomendar
        scenarios: Lista de cenÃ¡rios
        max_combinations: Limite de combinaÃ§Ãµes a avaliar
    
    Returns:
        Recommendation: RecomendaÃ§Ã£o Ã³tima
    
    Complexity:
        O(C(n, k) Ã— m) onde:
        - C(n, k) = combinaÃ§Ãµes de n skills tomadas k a k
        - m = nÃºmero de cenÃ¡rios
        
        Para n=10, k=3: C(10, 3) = 120 combinaÃ§Ãµes
    """
    if scenarios is None:
        scenarios = load_market_scenarios()
    
    # Skills disponÃ­veis
    available_skills = []
    for skill_id in graph.nodes:
        if skill_id in current_skills:
            continue
        
        prereqs = graph.get_prerequisites(skill_id)
        if all(prereq in current_skills for prereq in prereqs):
            available_skills.append(skill_id)
    
    if len(available_skills) < n_recommendations:
        # NÃ£o hÃ¡ skills suficientes, usa guloso
        return greedy_recommendation_with_lookahead(
            graph, current_skills, n_recommendations, scenarios, lookahead_depth=1
        )
    
    # Enumera todas as combinaÃ§Ãµes de n_recommendations skills
    best_combination = None
    best_expected_value = -float('inf')
    best_per_scenario = {}
    
    combinations_evaluated = 0
    
    for combination in combinations(available_skills, n_recommendations):
        if combinations_evaluated >= max_combinations:
            break
        
        combinations_evaluated += 1
        
        # Calcula valor esperado desta combinaÃ§Ã£o
        total_expected = 0.0
        per_scenario = {scenario.name: 0.0 for scenario in scenarios}
        
        valid_combination = True
        for skill_id in combination:
            # Verifica se todos os skills na combinaÃ§Ã£o tÃªm prÃ©-reqs satisfeitos
            # considerando os outros skills da combinaÃ§Ã£o
            temp_acquired = current_skills | set(combination[:combination.index(skill_id)])
            prereqs = graph.get_prerequisites(skill_id)
            
            if not all(prereq in temp_acquired for prereq in prereqs):
                valid_combination = False
                break
            
            metadata = graph.get_metadata(skill_id)
            base_value = metadata['valor']
            expected, values_per_scenario = calculate_expected_value(
                skill_id, base_value, scenarios
            )
            
            total_expected += expected
            for scenario_name, value in values_per_scenario.items():
                per_scenario[scenario_name] += value
        
        if not valid_combination:
            continue
        
        # Atualiza melhor combinaÃ§Ã£o
        if total_expected > best_expected_value:
            best_expected_value = total_expected
            best_combination = combination
            best_per_scenario = per_scenario
    
    if best_combination is None:
        # Fallback para guloso
        return greedy_recommendation_with_lookahead(
            graph, current_skills, n_recommendations, scenarios, lookahead_depth=1
        )
    
    # ConstrÃ³i resultado
    details = []
    for skill_id in best_combination:
        metadata = graph.get_metadata(skill_id)
        base_value = metadata['valor']
        expected, values_per_scenario = calculate_expected_value(
            skill_id, base_value, scenarios
        )
        
        details.append({
            'skill_id': skill_id,
            'nome': metadata['nome'],
            'immediate_value': expected,
            'future_value': 0.0,
            'total_score': expected,
            'base_value': base_value,
            'tempo_horas': metadata['tempo_horas'],
            'values_per_scenario': values_per_scenario
        })
    
    reasoning = generate_reasoning(details, scenarios)
    
    return Recommendation(
        skills_recommended=list(best_combination),
        expected_value=best_expected_value,
        expected_value_per_scenario=best_per_scenario,
        reasoning=reasoning,
        details=details
    )


def compare_recommendation_methods(
    graph: SkillGraph,
    current_skills: Set[str],
    n_recommendations: int = N_RECOMMENDATIONS,
    scenarios: Optional[List[MarketScenario]] = None
) -> Dict[str, Any]:
    """
    Compara diferentes mÃ©todos de recomendaÃ§Ã£o.
    
    Args:
        graph: Grafo de habilidades
        current_skills: Skills jÃ¡ adquiridas
        n_recommendations: NÃºmero de recomendaÃ§Ãµes
        scenarios: CenÃ¡rios de mercado
    
    Returns:
        Dict com comparaÃ§Ã£o dos mÃ©todos
    """
    if scenarios is None:
        scenarios = load_market_scenarios()
    
    # MÃ©todo 1: Guloso simples (sem look-ahead)
    greedy_simple = greedy_recommendation_with_lookahead(
        graph, current_skills, n_recommendations, scenarios, lookahead_depth=0
    )
    
    # MÃ©todo 2: Guloso com look-ahead
    greedy_lookahead = greedy_recommendation_with_lookahead(
        graph, current_skills, n_recommendations, scenarios, lookahead_depth=2
    )
    
    # MÃ©todo 3: DP exaustivo
    dp_exhaustive = dp_recommendation_exhaustive(
        graph, current_skills, n_recommendations, scenarios
    )
    
    return {
        'greedy_simple': greedy_simple.to_dict(),
        'greedy_lookahead': greedy_lookahead.to_dict(),
        'dp_exhaustive': dp_exhaustive.to_dict(),
        'comparison': {
            'greedy_simple_value': greedy_simple.expected_value,
            'greedy_lookahead_value': greedy_lookahead.expected_value,
            'dp_exhaustive_value': dp_exhaustive.expected_value,
            'best_method': max([
                ('greedy_simple', greedy_simple.expected_value),
                ('greedy_lookahead', greedy_lookahead.expected_value),
                ('dp_exhaustive', dp_exhaustive.expected_value)
            ], key=lambda x: x[1])[0]
        }
    }


@measure_performance
def solve_complete(
    graph: SkillGraph,
    current_skills: Set[str],
    n_recommendations: int = N_RECOMMENDATIONS,
    horizon_years: int = RECOMMENDATION_HORIZON_YEARS
) -> Dict[str, Any]:
    """
    Resolve o Desafio 5 COMPLETO:
    1. Carrega cenÃ¡rios de mercado
    2. Calcula recomendaÃ§Ãµes com look-ahead
    3. Compara diferentes mÃ©todos
    4. Analisa impacto de cada cenÃ¡rio
    
    Esta Ã© a funÃ§Ã£o principal do Desafio 5.
    
    Args:
        graph: Grafo de habilidades
        current_skills: Skills jÃ¡ adquiridas pelo profissional
        n_recommendations: NÃºmero de skills a recomendar (2-3)
        horizon_years: Horizonte de planejamento (anos)
    
    Returns:
        Dict com TODOS os resultados do Desafio 5
    """
    print("\n" + "=" * 70)
    print("ğŸ¯ DESAFIO 5 - RECOMENDAÃ‡ÃƒO DE PRÃ“XIMAS HABILIDADES")
    print("=" * 70)
    
    print(f"\nObjetivo: Recomendar {n_recommendations} habilidades")
    print(f"Horizonte: {horizon_years} anos")
    print(f"Skills atuais: {', '.join(sorted(current_skills)) if current_skills else 'Nenhuma'}")
    
    # 1. Carrega cenÃ¡rios de mercado
    print("\nğŸ“Š FASE 1: CenÃ¡rios de Mercado")
    print("-" * 70)
    
    scenarios = load_market_scenarios()
    
    print(f"\nğŸ“ˆ {len(scenarios)} cenÃ¡rios carregados:")
    for scenario in scenarios:
        print(f"   â€¢ {scenario.name}: {scenario.probability*100:.0f}% de probabilidade")
        boost_skills = [f"{sid}(+{(mult-1)*100:.0f}%)" 
                       for sid, mult in scenario.boosts.items() if mult > 1.0]
        if boost_skills:
            print(f"     Boosts: {', '.join(boost_skills[:3])}")
    
    # 2. RecomendaÃ§Ã£o com look-ahead
    print("\nğŸ“Š FASE 2: RecomendaÃ§Ã£o com Look-Ahead")
    print("-" * 70)
    
    recommendation = greedy_recommendation_with_lookahead(
        graph, current_skills, n_recommendations, scenarios, lookahead_depth=2
    )
    
    print(f"\nâœ… RecomendaÃ§Ã£o:")
    print(f"   Skills: {' â†’ '.join(recommendation.skills_recommended)}")
    print(f"   E[Valor] = {recommendation.expected_value:.2f}")
    
    print(f"\nğŸ“‹ Detalhes:")
    for i, detail in enumerate(recommendation.details, 1):
        print(f"\n   {i}. {detail['skill_id']} - {detail['nome']}")
        print(f"      Valor base: {detail['base_value']}")
        print(f"      E[Valor]: {detail['immediate_value']:.2f}")
        print(f"      Tempo: {detail['tempo_horas']}h")
        
        # Mostra valores por cenÃ¡rio
        best_scenario = max(detail['values_per_scenario'].items(), key=lambda x: x[1])
        print(f"      Melhor em: {best_scenario[0]} (V={best_scenario[1]:.2f})")
    
    print(f"\nğŸ’¡ {recommendation.reasoning}")
    
    # 3. ComparaÃ§Ã£o de mÃ©todos
    print("\nğŸ“Š FASE 3: ComparaÃ§Ã£o de MÃ©todos")
    print("-" * 70)
    
    comparison = compare_recommendation_methods(
        graph, current_skills, n_recommendations, scenarios
    )
    
    print(f"\nğŸ“ˆ ComparaÃ§Ã£o:")
    print(f"   Guloso simples:     E[V] = {comparison['comparison']['greedy_simple_value']:.2f}")
    print(f"   Guloso + lookahead: E[V] = {comparison['comparison']['greedy_lookahead_value']:.2f}")
    print(f"   DP exaustivo:       E[V] = {comparison['comparison']['dp_exhaustive_value']:.2f}")
    print(f"   Melhor mÃ©todo: {comparison['comparison']['best_method']}")
    
    # 4. AnÃ¡lise por cenÃ¡rio
    print("\nğŸ“Š FASE 4: AnÃ¡lise por CenÃ¡rio de Mercado")
    print("-" * 70)
    
    print(f"\nğŸ’° Valor esperado por cenÃ¡rio:")
    for scenario_name, value in recommendation.expected_value_per_scenario.items():
        scenario_obj = next(s for s in scenarios if s.name == scenario_name)
        print(f"   â€¢ {scenario_name}: {value:.2f} (prob={scenario_obj.probability*100:.0f}%)")
    
    return {
        'recommendation': recommendation.to_dict(),
        'scenarios': [
            {
                'name': s.name,
                'probability': s.probability,
                'boosts': s.boosts
            } for s in scenarios
        ],
        'comparison': comparison,
        'horizon_years': horizon_years,
        'n_recommendations': n_recommendations,
        'current_skills': list(current_skills)
    }


def print_recommendation_details(recommendation: Recommendation) -> None:
    """
    Imprime detalhes completos de uma recomendaÃ§Ã£o.
    
    Args:
        recommendation: RecomendaÃ§Ã£o a imprimir
    """
    print("\n" + "=" * 70)
    print("ğŸ“‹ DETALHES DA RECOMENDAÃ‡ÃƒO")
    print("=" * 70)
    
    print(f"\nğŸ¯ Habilidades recomendadas: {' â†’ '.join(recommendation.skills_recommended)}")
    print(f"\nğŸ“Š Valor esperado total: {recommendation.expected_value:.2f}")
    
    print(f"\nğŸ’¡ Justificativa:")
    for line in recommendation.reasoning.split('\n'):
        print(f"   {line}")
    
    print(f"\nğŸ“ Detalhamento:")
    for i, detail in enumerate(recommendation.details, 1):
        print(f"\n   {i}. {detail['skill_id']} - {detail['nome']}")
        print(f"      â€¢ Valor base: {detail['base_value']}")
        print(f"      â€¢ Valor esperado: {detail['immediate_value']:.2f}")
        print(f"      â€¢ Valor futuro: {detail['future_value']:.2f}")
        print(f"      â€¢ Score total: {detail['total_score']:.2f}")
        print(f"      â€¢ Tempo: {detail['tempo_horas']}h")
    
    print("=" * 70)