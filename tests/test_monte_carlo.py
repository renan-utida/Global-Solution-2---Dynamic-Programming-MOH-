"""
Testes para o m√≥dulo monte_carlo.py

Valida:
- Simula√ß√£o de incerteza
- Gera√ß√£o de cen√°rios
- C√°lculo de estat√≠sticas
- Compara√ß√£o determin√≠stico vs estoc√°stico
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import numpy as np
from src.monte_carlo import (
    simulate_value_with_uncertainty,
    generate_scenarios,
    calculate_statistics,
    run_monte_carlo,
    compare_deterministic_vs_stochastic,
    MonteCarloResult,
    quick_monte_carlo,
    print_monte_carlo_summary
)
from src.graph_structures import load_skills_from_json
from src.config import SKILLS_DATASET_FILE


def test_simulate_value_with_uncertainty():
    """Testa simula√ß√£o de valor com incerteza."""
    print("\n" + "=" * 70)
    print("üß™ TESTE 1: Simula√ß√£o de Valor com Incerteza")
    print("=" * 70)
    
    base_value = 100
    uncertainty = 0.10  # ¬±10%
    n_samples = 10000
    
    print(f"\nValor base: {base_value}")
    print(f"Incerteza: ¬±{uncertainty * 100}%")
    print(f"Range esperado: [{base_value * (1 - uncertainty)}, {base_value * (1 + uncertainty)}]")
    print(f"Amostras: {n_samples:,}")
    
    # Gera amostras
    samples = [
        simulate_value_with_uncertainty(base_value, uncertainty, distribution='uniform', seed=None)
        for _ in range(n_samples)
    ]
    
    # Calcula estat√≠sticas
    mean = np.mean(samples)
    std = np.std(samples)
    min_val = np.min(samples)
    max_val = np.max(samples)
    
    print(f"\nüìä Resultados:")
    print(f"   ‚Ä¢ M√©dia: {mean:.2f} (esperado: ~{base_value})")
    print(f"   ‚Ä¢ Desvio padr√£o: {std:.2f}")
    print(f"   ‚Ä¢ M√≠nimo: {min_val:.2f} (esperado: ~{base_value * 0.9})")
    print(f"   ‚Ä¢ M√°ximo: {max_val:.2f} (esperado: ~{base_value * 1.1})")
    
    # Valida√ß√µes
    assert 90 <= min_val <= 92, f"M√≠nimo fora do esperado: {min_val}"
    assert 108 <= max_val <= 110, f"M√°ximo fora do esperado: {max_val}"
    assert 98 <= mean <= 102, f"M√©dia fora do esperado: {mean}"
    
    print("\n‚úÖ Teste 1: PASSOU - Distribui√ß√£o uniforme correta")
    return True


def test_generate_scenarios():
    """Testa gera√ß√£o de cen√°rios estoc√°sticos."""
    print("\n" + "=" * 70)
    print("üß™ TESTE 2: Gera√ß√£o de Cen√°rios")
    print("=" * 70)
    
    # Dataset simplificado para teste
    skills_dict = {
        'S1': {'nome': 'Python', 'tempo_horas': 80, 'valor': 3, 'complexidade': 4, 'pre_requisitos': []},
        'S3': {'nome': 'Algoritmos', 'tempo_horas': 100, 'valor': 7, 'complexidade': 8, 'pre_requisitos': ['S1']}
    }
    
    n_scenarios = 100
    
    print(f"Gerando {n_scenarios} cen√°rios...")
    print(f"Valor base S1: {skills_dict['S1']['valor']}")
    print(f"Valor base S3: {skills_dict['S3']['valor']}")
    
    scenarios = generate_scenarios(skills_dict, n_scenarios=n_scenarios, uncertainty=0.10, seed=42)
    
    print(f"\n‚úÖ Cen√°rios gerados: {len(scenarios)}")
    
    # Valida estrutura
    assert len(scenarios) == n_scenarios
    assert 'S1' in scenarios[0]
    assert 'S3' in scenarios[0]
    
    # Coleta valores de S1 em todos os cen√°rios
    s1_values = [scenario['S1']['valor'] for scenario in scenarios]
    
    print(f"\nüìä Varia√ß√£o em S1:")
    print(f"   ‚Ä¢ M√≠nimo: {min(s1_values):.2f}")
    print(f"   ‚Ä¢ M√°ximo: {max(s1_values):.2f}")
    print(f"   ‚Ä¢ M√©dia: {np.mean(s1_values):.2f}")
    
    # Valida que valores variam
    assert len(set(s1_values)) > 50, "Valores n√£o est√£o variando suficientemente"
    assert all(2.7 <= v <= 3.3 for v in s1_values), "Valores fora do range ¬±10%"
    
    # Valida que tempo n√£o varia (por padr√£o)
    s1_times = [scenario['S1']['tempo_horas'] for scenario in scenarios]
    assert all(t == 80 for t in s1_times), "Tempo n√£o deveria variar por padr√£o"
    
    print("\n‚úÖ Teste 2: PASSOU - Cen√°rios gerados corretamente")
    return True


def test_calculate_statistics():
    """Testa c√°lculo de estat√≠sticas."""
    print("\n" + "=" * 70)
    print("üß™ TESTE 3: C√°lculo de Estat√≠sticas")
    print("=" * 70)
    
    # Dados de teste conhecidos
    results = [100, 105, 95, 110, 90, 102, 98, 107, 93, 101]
    
    print(f"Dados de teste: {results}")
    
    stats = calculate_statistics(results)
    
    print(f"\nüìä Estat√≠sticas calculadas:")
    print(f"   ‚Ä¢ M√©dia: {stats.expected_value:.2f}")
    print(f"   ‚Ä¢ Desvio padr√£o: {stats.std_deviation:.2f}")
    print(f"   ‚Ä¢ M√≠nimo: {stats.min_value:.2f}")
    print(f"   ‚Ä¢ M√°ximo: {stats.max_value:.2f}")
    print(f"   ‚Ä¢ Mediana: {stats.median:.2f}")
    print(f"   ‚Ä¢ Q1: {stats.percentile_25:.2f}")
    print(f"   ‚Ä¢ Q3: {stats.percentile_75:.2f}")
    
    # Valida√ß√µes
    assert isinstance(stats, MonteCarloResult)
    assert stats.n_scenarios == 10
    assert 98 <= stats.expected_value <= 102  # M√©dia pr√≥xima de 100
    assert stats.min_value == 90
    assert stats.max_value == 110
    
    # Valida intervalo de confian√ßa
    ci_lower, ci_upper = stats.confidence_interval_95
    print(f"   ‚Ä¢ IC 95%: [{ci_lower:.2f}, {ci_upper:.2f}]")
    assert ci_lower < stats.expected_value < ci_upper
    
    print("\n‚úÖ Teste 3: PASSOU - Estat√≠sticas corretas")
    return True


def test_run_monte_carlo():
    """Testa execu√ß√£o completa de Monte Carlo."""
    print("\n" + "=" * 70)
    print("üß™ TESTE 4: Execu√ß√£o Monte Carlo Completa")
    print("=" * 70)
    
    # Fun√ß√£o de otimiza√ß√£o simples para teste
    def simple_optimization(skills_dict):
        """Soma todos os valores (otimiza√ß√£o trivial)."""
        return sum(skill['valor'] for skill in skills_dict.values())
    
    # Dataset simplificado
    skills_dict = {
        'S1': {'nome': 'Python', 'tempo_horas': 80, 'valor': 3, 'complexidade': 4, 'pre_requisitos': []},
        'S2': {'nome': 'SQL', 'tempo_horas': 60, 'valor': 4, 'complexidade': 3, 'pre_requisitos': []},
    }
    
    print("Fun√ß√£o de otimiza√ß√£o: soma de todos os valores")
    print(f"Valor determin√≠stico: {simple_optimization(skills_dict)}")
    
    # Gera cen√°rios
    n_scenarios = 500
    print(f"\nGerando {n_scenarios} cen√°rios...")
    scenarios = generate_scenarios(skills_dict, n_scenarios=n_scenarios, uncertainty=0.10, seed=42)
    
    # Executa Monte Carlo
    print("Executando Monte Carlo...")
    result = run_monte_carlo(simple_optimization, scenarios)
    
    print(f"\nüìä Resultado:")
    print(f"   ‚Ä¢ E[Soma]: {result.expected_value:.2f}")
    print(f"   ‚Ä¢ œÉ: {result.std_deviation:.2f}")
    print(f"   ‚Ä¢ Range: [{result.min_value:.2f}, {result.max_value:.2f}]")
    
    # Valida√ß√µes
    assert result.n_scenarios == n_scenarios
    # Esperado: ~7 (3 + 4)
    assert 6.5 <= result.expected_value <= 7.5
    
    print("\n‚úÖ Teste 4: PASSOU - Monte Carlo executado com sucesso")
    return True


def test_compare_deterministic_vs_stochastic():
    """Testa compara√ß√£o determin√≠stico vs estoc√°stico."""
    print("\n" + "=" * 70)
    print("üß™ TESTE 5: Compara√ß√£o Determin√≠stico vs Estoc√°stico")
    print("=" * 70)
    
    # Resultado determin√≠stico
    deterministic = 30.0
    
    # Resultado estoc√°stico (simulado)
    stochastic_results = [29, 31, 30, 32, 28, 30, 31, 29, 30, 31]
    stochastic = calculate_statistics(stochastic_results)
    
    print(f"Valor determin√≠stico: {deterministic}")
    print(f"Valor estoc√°stico (E[X]): {stochastic.expected_value:.2f} ¬± {stochastic.std_deviation:.2f}")
    
    # Compara
    comparison = compare_deterministic_vs_stochastic(deterministic, stochastic)
    
    print(f"\nüìä Compara√ß√£o:")
    print(f"   ‚Ä¢ Diferen√ßa: {comparison['difference']:.2f}")
    print(f"   ‚Ä¢ Erro relativo: {comparison['relative_error_percent']:.2f}%")
    print(f"   ‚Ä¢ Dentro do IC 95%: {comparison['deterministic_within_95ci']}")
    
    print(f"\nüí¨ Interpreta√ß√£o:")
    print(f"   {comparison['interpretation']}")
    
    # Valida√ß√µes
    assert 'difference' in comparison
    assert 'relative_error_percent' in comparison
    assert 'deterministic_within_95ci' in comparison
    
    print("\n‚úÖ Teste 5: PASSOU - Compara√ß√£o realizada")
    return True


def test_with_real_dataset():
    """Testa com dataset real de 12 habilidades."""
    print("\n" + "=" * 70)
    print("üß™ TESTE 6: Monte Carlo com Dataset Real")
    print("=" * 70)
    
    print(f"Carregando dataset de: {SKILLS_DATASET_FILE}")
    skills = load_skills_from_json(SKILLS_DATASET_FILE)
    
    print(f"‚úÖ Dataset carregado: {len(skills)} habilidades")
    
    # Fun√ß√£o de otimiza√ß√£o simples: soma todos os valores
    def sum_all_values(skills_dict):
        return sum(skill['valor'] for skill in skills_dict.values())
    
    deterministic = sum_all_values(skills)
    print(f"\nValor determin√≠stico (soma): {deterministic}")
    
    # Executa Monte Carlo (poucos cen√°rios para teste r√°pido)
    print("\nExecutando Monte Carlo (100 cen√°rios)...")
    result = quick_monte_carlo(
        sum_all_values,
        skills,
        n_scenarios=100,
        uncertainty=0.10,
        seed=42
    )
    
    print_monte_carlo_summary(result)
    
    # Valida√ß√µes
    assert result.n_scenarios == 100
    # Valor esperado deve ser pr√≥ximo do determin√≠stico
    assert abs(result.expected_value - deterministic) < 5
    
    print("\n‚úÖ Teste 6: PASSOU - Monte Carlo funciona com dataset real")
    return True


def main():
    """Executa todos os testes."""
    print("\n" + "=" * 70)
    print("üß™ VALIDA√á√ÉO DE monte_carlo.py")
    print("=" * 70)
    
    tests = [
        ("Simula√ß√£o de Valor com Incerteza", test_simulate_value_with_uncertainty),
        ("Gera√ß√£o de Cen√°rios", test_generate_scenarios),
        ("C√°lculo de Estat√≠sticas", test_calculate_statistics),
        ("Execu√ß√£o Monte Carlo Completa", test_run_monte_carlo),
        ("Compara√ß√£o Determin√≠stico vs Estoc√°stico", test_compare_deterministic_vs_stochastic),
        ("Monte Carlo com Dataset Real", test_with_real_dataset),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append(result)
        except Exception as e:
            print(f"\n‚ùå ERRO CR√çTICO em '{test_name}': {e}")
            import traceback
            traceback.print_exc()
            results.append(False)
    
    # Resumo
    print("\n" + "=" * 70)
    print("üìä RESUMO DOS TESTES - monte_carlo.py")
    print("=" * 70)
    
    total_tests = len(tests)
    passed_tests = sum(results)
    
    for i, (test_name, _) in enumerate(tests):
        status = "‚úÖ PASSOU" if results[i] else "‚ùå FALHOU"
        print(f"{status} - {test_name}")
    
    print(f"\nüìà Resultados: {passed_tests}/{total_tests} testes passaram")
    
    if passed_tests == total_tests:
        print("\nüéâ monte_carlo.py VALIDADO COM SUCESSO!")
        print("‚úÖ Pronto para usar no Desafio 1")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total_tests - passed_tests} teste(s) falharam.")
        return 1


if __name__ == '__main__':
    sys.exit(main())